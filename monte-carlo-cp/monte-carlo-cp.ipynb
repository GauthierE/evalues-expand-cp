{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# transformations for the test dataset\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# load the CIFAR-10H annotations\n",
    "cifar10h_file = \"data/cifar10h-probs.npy\"  # download from https://github.com/jcpeterson/cifar-10h\n",
    "cifar10h_probs = np.load(cifar10h_file)\n",
    "\n",
    "# note: CIFAR-10H annotations correspond to CIFAR-10 test dataset\n",
    "\n",
    "# load CIFAR-10 test dataset\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test\n",
    ")\n",
    "\n",
    "# CIFAR-10 class names\n",
    "class_names = [\n",
    "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "# define a function to unnormalize and convert the image back to a NumPy array\n",
    "def unnormalize_image(image_tensor):\n",
    "    mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "    std = np.array([0.2023, 0.1994, 0.2010])\n",
    "    image = image_tensor.numpy().transpose((1, 2, 0))  # convert from (C, H, W) to (H, W, C)\n",
    "    image = std * image + mean  # unnormalize\n",
    "    image = np.clip(image, 0, 1)  # clip to valid range [0, 1]\n",
    "    return image\n",
    "\n",
    "# plot image with its label distribution\n",
    "def plot_image_with_distribution(image_tensor, label_distribution):\n",
    "    # unnormalize and convert image to NumPy format\n",
    "    image = unnormalize_image(image_tensor)\n",
    "    \n",
    "    # plot image and label distribution\n",
    "    plt.figure(figsize=(6, 3))\n",
    "\n",
    "    # image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Image\")\n",
    "\n",
    "    # label distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(class_names, label_distribution, color='blue')\n",
    "    plt.title(\"Label Distribution\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# show examples for specific indices\n",
    "for i in [151, 165]:  # examples with ambiguous ground truth\n",
    "    image_tensor, _ = testset[i]\n",
    "    plot_image_with_distribution(image_tensor, cifar10h_probs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# model\n",
    "net = EfficientNetB0()\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "weights_path = \"weights/EfficientNetB0_0.1_100_512_SGD_1\" \n",
    "\n",
    "# load weights into the model\n",
    "net.load_state_dict(torch.load(weights_path))\n",
    "print(\"Model weights loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.3\n",
    "\n",
    "testset_size = len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the samples with ambiguous ground truth only\n",
    "valid_indices = [i for i, dist in enumerate(cifar10h_probs) if np.sum(np.array(dist) > 0.1) >= 2]\n",
    "\n",
    "# filter the test set and label distributions\n",
    "filtered_testset = torch.utils.data.Subset(testset, valid_indices)\n",
    "filtered_cifar10h_probs = cifar10h_probs[valid_indices]\n",
    "\n",
    "# check the size of the filtered dataset\n",
    "print(f\"Filtered test set size: {len(filtered_testset)}\")\n",
    "\n",
    "# function to get CIFAR-10H label distributions for a subset\n",
    "def get_filtered_cifar10h_distributions(subset):\n",
    "    # retrieve the label distributions for the valid indices in the subset\n",
    "    return [filtered_cifar10h_probs[idx] for idx in subset.indices]\n",
    "\n",
    "filtered_calibration_size = int(0.3 * len(filtered_testset))\n",
    "filtered_final_test_size = len(filtered_testset) - filtered_calibration_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "list_results = [] # list of 2 dictionaries, for m=1 and m=20\n",
    "# each dict has coverage and length averaged over Niter values\n",
    "\n",
    "for m in [1,20]:\n",
    "    print(f'### m = {m}')\n",
    "\n",
    "    list_coverage = []\n",
    "    list_coverage_p = []\n",
    "    list_avg_len = []\n",
    "    list_avg_len_p = []\n",
    "    list_avg_len_correct = []\n",
    "    list_avg_len_correct_p = []\n",
    "    list_avg_len_wrong = []\n",
    "    list_avg_len_wrong_p = []\n",
    "\n",
    "    Niter = 200\n",
    "\n",
    "    for iter in range(Niter):\n",
    "        print(iter)\n",
    "\n",
    "        # split the filtered test set\n",
    "        filtered_calibration_set, filtered_final_test_set = random_split(\n",
    "            filtered_testset,\n",
    "            [filtered_calibration_size, filtered_final_test_size],\n",
    "            generator=torch.Generator().manual_seed(iter)\n",
    "        )\n",
    "\n",
    "        # get label distributions for the filtered splits\n",
    "        filtered_calibration_label_distributions = get_filtered_cifar10h_distributions(filtered_calibration_set)\n",
    "        filtered_final_test_label_distributions = get_filtered_cifar10h_distributions(filtered_final_test_set)\n",
    "\n",
    "        # store the scores\n",
    "        calibration_scores = []\n",
    "\n",
    "        # loop through each sample in the calibration set\n",
    "        with torch.no_grad(): \n",
    "            for i in range(filtered_calibration_size):\n",
    "                image, _ = filtered_calibration_set[i]  # get individual sample\n",
    "                image = image.unsqueeze(0).to(device)\n",
    "                label_distribution = filtered_calibration_label_distributions[i]\n",
    "                output = net(image)\n",
    "                for j in range(m):\n",
    "                    # draw an index based on the probabilities\n",
    "                    label = np.random.choice(len(label_distribution), p=label_distribution)\n",
    "                    label = torch.tensor(label).unsqueeze(0).to(device)\n",
    "                    loss = criterion(output, label)  # compute loss\n",
    "  \n",
    "                    calibration_scores.append(loss.item()) \n",
    "\n",
    "        # initialize the sums list with zeros\n",
    "        calibration_sums = [0] * m\n",
    "\n",
    "        # compute the sums for all j\n",
    "        for j in range(m):\n",
    "            calibration_sums[j] = sum(calibration_scores[j + i * m] for i in range(len(calibration_scores) // m))\n",
    "\n",
    "        quantile = np.quantile(calibration_scores, (np.ceil(m*(1-alpha)*(filtered_calibration_size+1))-1)/(m*filtered_calibration_size))\n",
    "\n",
    "        # initialize an empty list to store the conformal sets\n",
    "        conformal_sets = []\n",
    "        pconformal_sets = []\n",
    "\n",
    "        # loop through each sample in the final test set\n",
    "        with torch.no_grad(): \n",
    "            for i in range(filtered_final_test_size):\n",
    "                image, _ = filtered_final_test_set[i]  # get individual sample\n",
    "                image = image.unsqueeze(0).to(device)                \n",
    "                output = net(image) \n",
    "\n",
    "                conformal_set = [] # conformal set for final_test_set[i]\n",
    "                pconformal_set = []\n",
    "\n",
    "                for y in range(10): # possible labels y\n",
    "\n",
    "                    possible_label = torch.tensor(y).unsqueeze(0).to(device)\n",
    "\n",
    "                    loss = criterion(output, possible_label)\n",
    "\n",
    "                    # compute e-variable\n",
    "                    e_var = 0\n",
    "                    for k in range(m):\n",
    "                        e_var += (filtered_calibration_size+1)/m*loss / (calibration_sums[k] + loss)\n",
    "\n",
    "                    # conformal e-prediction criterion\n",
    "                    if e_var < 1/alpha :\n",
    "                        conformal_set.append(y)\n",
    "\n",
    "                    # conformal p-prediction criterion\n",
    "                    if loss <= quantile:\n",
    "                        pconformal_set.append(y)\n",
    "                \n",
    "                conformal_sets.append(conformal_set)\n",
    "                pconformal_sets.append(pconformal_set)\n",
    "\n",
    "        # compute coverage, conformal set size (size=length) \n",
    "        coverage = 0\n",
    "        coverage_p = 0\n",
    "        avg_len = 0\n",
    "        avg_len_p = 0\n",
    "\n",
    "        for i in range(filtered_final_test_size):\n",
    "            _, true_label = filtered_final_test_set[i]  # get individual sample\n",
    "            label_distribution = filtered_final_test_label_distributions[i]\n",
    "            label = np.random.choice(len(label_distribution), p=label_distribution)\n",
    "\n",
    "            if label in conformal_sets[i]:\n",
    "                coverage += 1\n",
    "            if label in pconformal_sets[i]:\n",
    "                coverage_p += 1\n",
    "            avg_len += len(conformal_sets[i])\n",
    "            avg_len_p += len(pconformal_sets[i])\n",
    "\n",
    "        coverage = coverage / filtered_final_test_size\n",
    "        coverage_p = coverage_p / filtered_final_test_size\n",
    "        avg_len = avg_len / filtered_final_test_size\n",
    "        avg_len_p = avg_len_p / filtered_final_test_size\n",
    "\n",
    "        list_coverage.append(coverage)\n",
    "        list_coverage_p.append(coverage_p)\n",
    "        list_avg_len.append(avg_len)\n",
    "        list_avg_len_p.append(avg_len_p)\n",
    "\n",
    "        avg_len_correct = 0\n",
    "        avg_len_correct_p = 0\n",
    "        nb_correct = 0\n",
    "\n",
    "        avg_len_wrong = 0\n",
    "        avg_len_wrong_p = 0\n",
    "        nb_wrong = 0\n",
    "\n",
    "        for i in range(filtered_final_test_size):\n",
    "            image, true_label = filtered_final_test_set[i]  # get individual sample\n",
    "            image = image.unsqueeze(0).to(device)\n",
    "            true_label = torch.tensor(true_label).unsqueeze(0).to(device)   \n",
    "            output = net(image) \n",
    "            prediction = torch.argmax(output)\n",
    "            if prediction == true_label:\n",
    "                nb_correct += 1\n",
    "                avg_len_correct += len(conformal_sets[i])\n",
    "                avg_len_correct_p += len(pconformal_sets[i])\n",
    "            else:\n",
    "                nb_wrong += 1\n",
    "                avg_len_wrong += len(conformal_sets[i])\n",
    "                avg_len_wrong_p += len(pconformal_sets[i])\n",
    "        avg_len_correct = avg_len_correct / nb_correct\n",
    "        avg_len_correct_p = avg_len_correct_p / nb_correct\n",
    "        avg_len_wrong = avg_len_wrong / nb_wrong\n",
    "        avg_len_wrong_p = avg_len_wrong_p / nb_wrong\n",
    "\n",
    "        list_avg_len_correct.append(avg_len_correct)\n",
    "        list_avg_len_correct_p.append(avg_len_correct_p)\n",
    "        list_avg_len_wrong.append(avg_len_wrong)\n",
    "        list_avg_len_wrong_p.append(avg_len_wrong_p)\n",
    "\n",
    "    # save\n",
    "    results = {\n",
    "        \"list_coverage\": list_coverage,\n",
    "        \"list_coverage_p\": list_coverage_p,\n",
    "        \"list_avg_len\": list_avg_len,\n",
    "        \"list_avg_len_p\": list_avg_len_p,\n",
    "        \"list_avg_len_correct\": list_avg_len_correct,\n",
    "        \"list_avg_len_correct_p\": list_avg_len_correct_p,\n",
    "        \"list_avg_len_wrong\": list_avg_len_wrong,\n",
    "        \"list_avg_len_wrong_p\": list_avg_len_wrong_p,\n",
    "    }\n",
    "\n",
    "    list_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# save results\n",
    "save_dir = \"output\"\n",
    "os.makedirs(save_dir, exist_ok=True) \n",
    "\n",
    "with open(os.path.join(save_dir, \"hist.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(list_results, f)\n",
    "\n",
    "print(f\"Results saved in {os.path.join(save_dir, 'hist.pkl')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
